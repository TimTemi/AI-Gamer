{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XmSGG4orUTX7",
    "outputId": "d428a822-e5d6-4326-8422-6b0028e6b380"
   },
   "outputs": [],
   "source": [
    "#pip install gymnasium\n",
    "#pip install stable-baselines3[extra]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_axp7tInITj_"
   },
   "source": [
    "# importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bu3nH1OnDm88",
    "outputId": "5e8a627a-04d7-4b63-e133-c890b724d1d7"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack\n",
    "from stable_baselines3.dqn.policies import CnnPolicy\n",
    "from gymnasium.wrappers import FrameStack, ResizeObservation\n",
    "from gymnasium.utils.save_video import save_video\n",
    "from PIL import Image\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Klfgf7Hu9DNw"
   },
   "source": [
    "# CartPole Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "TfSErqIyqlVU"
   },
   "outputs": [],
   "source": [
    "class CartPoleDQNAgent:\n",
    "    def __init__(self, name=None, env_name=None, eval_freq=20000, buffer_size=1000):\n",
    "        self.name = name\n",
    "        self.env_name = env_name\n",
    "        self.policy = \"MlpPolicy\"\n",
    "        self.eval_freq = eval_freq\n",
    "        self.buffer_size = buffer_size\n",
    "        self.log_path = os.path.join('Training/DQN_' + self.name + '_Log')\n",
    "        self.save_path = os.path.join('Saved_Models/DQN_' + self.name + '_Model')\n",
    "        self.env = self.make_environment()\n",
    "        self.model = self._build_dqn()\n",
    "\n",
    "    def make_environment(self):\n",
    "        env = gym.make(self.env_name, render_mode=\"rgb_array\")\n",
    "        env = DummyVecEnv([lambda: env])\n",
    "        return env\n",
    "\n",
    "    def _build_dqn(self):\n",
    "        model = DQN(policy=self.policy, env=self.env, verbose=0, tensorboard_log=self.log_path, buffer_size=self.buffer_size)\n",
    "        return model\n",
    "\n",
    "    def _play_one_episode(self):\n",
    "        obs = self.env.reset()\n",
    "        done = False\n",
    "        score = 0\n",
    "\n",
    "        while not done:\n",
    "            action = self.env.action_space.sample()\n",
    "            obs, reward, done, _ = self.env.step([action])\n",
    "            score += reward\n",
    "\n",
    "        return score\n",
    "\n",
    "    def play_episodes(self, num_episodes=10, play_type=\"random\"):\n",
    "        if play_type == \"random\":\n",
    "            print(f\"Playing the {self.name} game randomly for {num_episodes} episodes\")\n",
    "            scores = [self._play_one_episode() for _ in range(num_episodes)]\n",
    "            for episode, score in enumerate(scores, 1):\n",
    "                print(f\"Episode {episode}: {score}\")\n",
    "\n",
    "        if play_type == \"predict\":\n",
    "            episode_rewards = []\n",
    "            frames = []\n",
    "\n",
    "            for episode in range(num_episodes):\n",
    "                obs = self.env.reset()\n",
    "                done = False\n",
    "                score = 0\n",
    "\n",
    "                while not done:\n",
    "                    action, _ = self.model.predict(obs)\n",
    "                    obs, reward, done, *info = self.env.step(action)\n",
    "                    score += reward\n",
    "                    frame = Image.fromarray(self.env.render())\n",
    "                    frame = np.array(frame)\n",
    "                    frames.append(frame)\n",
    "\n",
    "                episode_rewards.append(score)\n",
    "\n",
    "                print(f\"Episode {episode+1}: {score}\")\n",
    "\n",
    "            video_path = os.path.join(self.save_path, self.name + \"_Agent_play\")\n",
    "\n",
    "            save_video(frames, video_path, fps=30, name_prefix=f\"{self.name}-agent-play\")\n",
    "\n",
    "    def train(self, time_steps=None, stop_value=None):\n",
    "        stop_callback = StopTrainingOnRewardThreshold(reward_threshold=stop_value, verbose=0)\n",
    "        eval_callback = EvalCallback(self.env, callback_on_new_best=stop_callback, eval_freq=self.eval_freq, best_model_save_path=self.save_path)\n",
    "        self.model.learn(total_timesteps=time_steps, callback=eval_callback)\n",
    "\n",
    "    def evaluate_policy(self, episodes=None):\n",
    "        mean_reward, reward_std = evaluate_policy(self.model, self.env, n_eval_episodes=episodes)\n",
    "        print(f\"Mean reward over {episodes} episodes is {mean_reward} with a standard deviation of {reward_std}\")\n",
    "\n",
    "    def close_env(self):\n",
    "        self.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "n2SNSMa3qlYH"
   },
   "outputs": [],
   "source": [
    "#create the agent and create the environment\n",
    "CartPole_agent = CartPoleDQNAgent(name=\"CartPole\", env_name=\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uEjjHwaZAQqZ",
    "outputId": "f0e907d2-5c21-403e-d700-b7d8f4eaa32a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Playing the CartPole game randomly for 20 episodes\n",
      "Episode 1: [15.]\n",
      "Episode 2: [26.]\n",
      "Episode 3: [9.]\n",
      "Episode 4: [17.]\n",
      "Episode 5: [20.]\n",
      "Episode 6: [15.]\n",
      "Episode 7: [40.]\n",
      "Episode 8: [36.]\n",
      "Episode 9: [14.]\n",
      "Episode 10: [18.]\n",
      "Episode 11: [9.]\n",
      "Episode 12: [10.]\n",
      "Episode 13: [42.]\n",
      "Episode 14: [20.]\n",
      "Episode 15: [15.]\n",
      "Episode 16: [22.]\n",
      "Episode 17: [35.]\n",
      "Episode 18: [16.]\n",
      "Episode 19: [22.]\n",
      "Episode 20: [44.]\n"
     ]
    }
   ],
   "source": [
    "#Play the cart pole game randomly for 20 episodes\n",
    "CartPole_agent.play_episodes(num_episodes=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VgBhDR1BAQuF",
    "outputId": "6912ad3c-f96c-4a9f-9f12-70dd18d8b9ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=20000, episode_reward=28.80 +/- 9.50\n",
      "Episode length: 28.80 +/- 9.50\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=23.20 +/- 3.31\n",
      "Episode length: 23.20 +/- 3.31\n",
      "Eval num_timesteps=60000, episode_reward=9.80 +/- 1.17\n",
      "Episode length: 9.80 +/- 1.17\n",
      "Eval num_timesteps=80000, episode_reward=9.40 +/- 0.49\n",
      "Episode length: 9.40 +/- 0.49\n",
      "Eval num_timesteps=100000, episode_reward=13.20 +/- 1.60\n",
      "Episode length: 13.20 +/- 1.60\n",
      "Eval num_timesteps=120000, episode_reward=14.40 +/- 1.62\n",
      "Episode length: 14.40 +/- 1.62\n",
      "Eval num_timesteps=140000, episode_reward=171.20 +/- 5.60\n",
      "Episode length: 171.20 +/- 5.60\n",
      "New best mean reward!\n",
      "Eval num_timesteps=160000, episode_reward=151.00 +/- 21.91\n",
      "Episode length: 151.00 +/- 21.91\n",
      "Eval num_timesteps=180000, episode_reward=21.20 +/- 3.49\n",
      "Episode length: 21.20 +/- 3.49\n",
      "Eval num_timesteps=200000, episode_reward=68.40 +/- 2.33\n",
      "Episode length: 68.40 +/- 2.33\n"
     ]
    }
   ],
   "source": [
    "#test out the agent with the cart pole game\n",
    "CartPole_agent.train(time_steps=200000, stop_value=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Yd9sDTVGAQxp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: [66.]\n",
      "Episode 2: [73.]\n",
      "Episode 3: [68.]\n",
      "Episode 4: [69.]\n",
      "Episode 5: [62.]\n",
      "Episode 6: [70.]\n",
      "Episode 7: [65.]\n",
      "Episode 8: [66.]\n",
      "Episode 9: [73.]\n",
      "Episode 10: [67.]\n",
      "Moviepy - Building video c:\\Users\\Steel\\Downloads\\my_atari_games\\Saved_Models\\DQN_CartPole_Model\\CartPole_Agent_play/CartPole-agent-play-episode-0.mp4.\n",
      "Moviepy - Writing video c:\\Users\\Steel\\Downloads\\my_atari_games\\Saved_Models\\DQN_CartPole_Model\\CartPole_Agent_play/CartPole-agent-play-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready c:\\Users\\Steel\\Downloads\\my_atari_games\\Saved_Models\\DQN_CartPole_Model\\CartPole_Agent_play/CartPole-agent-play-episode-0.mp4\n"
     ]
    }
   ],
   "source": [
    "#test out the agent with the cart pole game\n",
    "CartPole_agent.play_episodes(num_episodes=10, play_type=\"predict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "yQOB73FM1Yru"
   },
   "outputs": [],
   "source": [
    "#close the environment\n",
    "CartPole_agent.close_env()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yp4V6nG4GEGb"
   },
   "source": [
    "# DQNAgent for SpaceInvaders and Pac-Man"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "AxHI8VtwAQ0Q"
   },
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, name=None, env_name=None, eval_freq=20000, buffer_size=1000):\n",
    "        self.name = name\n",
    "        self.env_name = env_name\n",
    "        self.eval_freq = eval_freq\n",
    "        self.buffer_size = buffer_size\n",
    "        self.log_path = os.path.join('Training/DQN_' + self.name + '_Log')\n",
    "        self.save_path = os.path.join('Saved_Models/DQN_' + self.name + '_Model')\n",
    "        self.env = self.make_environment()\n",
    "        self.model = self._build_dqn()\n",
    "\n",
    "    def make_environment(self):\n",
    "        env = gym.make(self.env_name, render_mode=\"rgb_array\")\n",
    "        env = ResizeObservation(env, 84)\n",
    "        return env\n",
    "\n",
    "    def _build_dqn(self):\n",
    "        model = DQN(CnnPolicy, self.env, verbose=0, tensorboard_log=self.log_path, buffer_size=self.buffer_size)\n",
    "        return model\n",
    "\n",
    "    def _play_one_episode(self):\n",
    "        obs, _ = self.env.reset()\n",
    "        done = False\n",
    "        score = 0\n",
    "\n",
    "        while not done:\n",
    "            action = self.env.action_space.sample()\n",
    "            obs, reward, done, *info = self.env.step(action)\n",
    "            score += reward\n",
    "\n",
    "        return score\n",
    "\n",
    "    def play_episodes(self, num_episodes=10, play_type=\"random\"):\n",
    "        if play_type == \"random\":\n",
    "            print(f\"Playing the {self.name} game randomly for {num_episodes} episodes\")\n",
    "            scores = [self._play_one_episode() for _ in range(num_episodes)]\n",
    "            for episode, score in enumerate(scores, 1):\n",
    "                print(f\"Episode {episode}: {score}\")\n",
    "\n",
    "        if play_type == \"predict\":\n",
    "            episode_rewards = []\n",
    "            frames = []\n",
    "\n",
    "            for episode in range(num_episodes):\n",
    "                obs, _ = self.env.reset()\n",
    "                done = False\n",
    "                score = 0\n",
    "\n",
    "                while not done:\n",
    "                    action, _ = self.model.predict(obs)\n",
    "                    obs, reward, done, *info = self.env.step(action)\n",
    "                    score += reward\n",
    "                    frame = Image.fromarray(self.env.render())\n",
    "                    frame = np.array(frame)\n",
    "                    frames.append(frame)\n",
    "\n",
    "                episode_rewards.append(score)\n",
    "\n",
    "                print(f\"Episode {episode+1}: {score}\")\n",
    "\n",
    "            video_path = os.path.join(self.save_path, self.name + \"_Agent_play\")\n",
    "\n",
    "            save_video(frames, video_path, fps=30, name_prefix=f\"{self.name}-agent-play\")\n",
    "\n",
    "    def train(self, time_steps=None, stop_value=None):\n",
    "        stop_callback = StopTrainingOnRewardThreshold(reward_threshold=stop_value, verbose=0)\n",
    "        eval_callback = EvalCallback(self.env, callback_on_new_best=stop_callback, eval_freq=self.eval_freq, best_model_save_path=self.save_path)\n",
    "        self.model.learn(total_timesteps=time_steps, callback=eval_callback)\n",
    "\n",
    "    def evaluate_policy(self, episodes=None):\n",
    "        mean_reward, reward_std = evaluate_policy(self.model, self.env, n_eval_episodes=episodes)\n",
    "        print(f\"Mean reward over {episodes} episodes is {mean_reward} with a standard deviation of {reward_std}\")\n",
    "\n",
    "    def load_best_model(self):\n",
    "        best_model = DQN.load(self.save_path + \"/best_model\")\n",
    "        return best_model\n",
    "\n",
    "    def save_model(self):\n",
    "        return self.model.save(self.save_path)\n",
    "\n",
    "    def close_env(self):\n",
    "        self.env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lO63ctPA0v5g"
   },
   "source": [
    "# SpaceInvaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "mrpuXIvoHVZq"
   },
   "outputs": [],
   "source": [
    "#initialize the agent and create the environment\n",
    "SpaceInvaders_agent = DQNAgent(name=\"SpaceInvaders\", env_name=\"SpaceInvaders-v4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "2_PgT3VNHVkg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Playing the SpaceInvaders game randomly for 20 episodes\n",
      "Episode 1: 10.0\n",
      "Episode 2: 15.0\n",
      "Episode 3: 55.0\n",
      "Episode 4: 100.0\n",
      "Episode 5: 90.0\n",
      "Episode 6: 130.0\n",
      "Episode 7: 155.0\n",
      "Episode 8: 215.0\n",
      "Episode 9: 30.0\n",
      "Episode 10: 190.0\n",
      "Episode 11: 90.0\n",
      "Episode 12: 30.0\n",
      "Episode 13: 100.0\n",
      "Episode 14: 185.0\n",
      "Episode 15: 490.0\n",
      "Episode 16: 385.0\n",
      "Episode 17: 60.0\n",
      "Episode 18: 100.0\n",
      "Episode 19: 105.0\n",
      "Episode 20: 30.0\n"
     ]
    }
   ],
   "source": [
    "#Play the space invaders game randomly for 20 episodes\n",
    "SpaceInvaders_agent.play_episodes(num_episodes=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "x6oZsnOcHVss"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=20000, episode_reward=139.00 +/- 35.55\n",
      "Episode length: 792.20 +/- 35.24\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=178.00 +/- 118.98\n",
      "Episode length: 863.80 +/- 185.14\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=244.00 +/- 77.87\n",
      "Episode length: 980.40 +/- 301.04\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=120.00 +/- 66.63\n",
      "Episode length: 678.80 +/- 207.70\n",
      "Eval num_timesteps=100000, episode_reward=153.00 +/- 72.77\n",
      "Episode length: 788.80 +/- 307.79\n",
      "Eval num_timesteps=120000, episode_reward=161.00 +/- 29.56\n",
      "Episode length: 753.60 +/- 41.58\n",
      "Eval num_timesteps=140000, episode_reward=367.00 +/- 173.74\n",
      "Episode length: 835.20 +/- 200.02\n",
      "New best mean reward!\n",
      "Eval num_timesteps=160000, episode_reward=161.00 +/- 110.74\n",
      "Episode length: 752.00 +/- 146.21\n",
      "Eval num_timesteps=180000, episode_reward=73.00 +/- 38.94\n",
      "Episode length: 592.40 +/- 80.62\n",
      "Eval num_timesteps=200000, episode_reward=87.00 +/- 88.58\n",
      "Episode length: 621.80 +/- 158.45\n",
      "Eval num_timesteps=220000, episode_reward=88.00 +/- 67.57\n",
      "Episode length: 688.40 +/- 176.31\n",
      "Eval num_timesteps=240000, episode_reward=256.00 +/- 149.31\n",
      "Episode length: 922.00 +/- 271.12\n",
      "Eval num_timesteps=260000, episode_reward=215.00 +/- 102.76\n",
      "Episode length: 847.80 +/- 138.66\n",
      "Eval num_timesteps=280000, episode_reward=198.00 +/- 102.55\n",
      "Episode length: 781.40 +/- 62.50\n",
      "Eval num_timesteps=300000, episode_reward=406.00 +/- 161.97\n",
      "Episode length: 1119.40 +/- 450.59\n",
      "New best mean reward!\n",
      "Eval num_timesteps=320000, episode_reward=253.00 +/- 211.51\n",
      "Episode length: 973.40 +/- 235.71\n",
      "Eval num_timesteps=340000, episode_reward=60.00 +/- 12.25\n",
      "Episode length: 700.60 +/- 5.78\n",
      "Eval num_timesteps=360000, episode_reward=5.00 +/- 0.00\n",
      "Episode length: 834.00 +/- 167.18\n",
      "Eval num_timesteps=380000, episode_reward=313.00 +/- 203.09\n",
      "Episode length: 1052.60 +/- 306.11\n",
      "Eval num_timesteps=400000, episode_reward=187.00 +/- 44.34\n",
      "Episode length: 981.80 +/- 169.47\n",
      "Eval num_timesteps=420000, episode_reward=79.00 +/- 110.70\n",
      "Episode length: 791.80 +/- 241.35\n",
      "Eval num_timesteps=440000, episode_reward=107.00 +/- 22.72\n",
      "Episode length: 605.20 +/- 155.54\n",
      "Eval num_timesteps=460000, episode_reward=28.00 +/- 9.27\n",
      "Episode length: 671.00 +/- 123.16\n",
      "Eval num_timesteps=480000, episode_reward=13.00 +/- 13.27\n",
      "Episode length: 842.60 +/- 236.95\n",
      "Eval num_timesteps=500000, episode_reward=112.00 +/- 51.44\n",
      "Episode length: 557.20 +/- 96.85\n",
      "Eval num_timesteps=520000, episode_reward=216.00 +/- 102.44\n",
      "Episode length: 940.00 +/- 248.88\n",
      "Eval num_timesteps=540000, episode_reward=59.00 +/- 53.98\n",
      "Episode length: 591.00 +/- 106.14\n",
      "Eval num_timesteps=560000, episode_reward=136.00 +/- 38.39\n",
      "Episode length: 875.20 +/- 118.64\n",
      "Eval num_timesteps=580000, episode_reward=132.00 +/- 65.24\n",
      "Episode length: 671.20 +/- 289.49\n",
      "Eval num_timesteps=600000, episode_reward=21.00 +/- 12.41\n",
      "Episode length: 875.40 +/- 212.40\n",
      "Eval num_timesteps=620000, episode_reward=85.00 +/- 30.82\n",
      "Episode length: 509.80 +/- 122.66\n",
      "Eval num_timesteps=640000, episode_reward=105.00 +/- 108.44\n",
      "Episode length: 684.00 +/- 112.76\n",
      "Eval num_timesteps=660000, episode_reward=165.00 +/- 40.25\n",
      "Episode length: 673.00 +/- 254.57\n",
      "Eval num_timesteps=680000, episode_reward=145.00 +/- 46.04\n",
      "Episode length: 784.00 +/- 211.80\n",
      "Eval num_timesteps=700000, episode_reward=114.00 +/- 54.17\n",
      "Episode length: 661.20 +/- 273.65\n",
      "Eval num_timesteps=720000, episode_reward=254.00 +/- 79.02\n",
      "Episode length: 779.40 +/- 191.57\n",
      "Eval num_timesteps=740000, episode_reward=82.00 +/- 37.50\n",
      "Episode length: 797.00 +/- 327.29\n",
      "Eval num_timesteps=760000, episode_reward=56.00 +/- 42.47\n",
      "Episode length: 791.00 +/- 244.06\n",
      "Eval num_timesteps=780000, episode_reward=216.00 +/- 143.23\n",
      "Episode length: 920.80 +/- 135.63\n",
      "Eval num_timesteps=800000, episode_reward=115.00 +/- 78.17\n",
      "Episode length: 584.00 +/- 189.36\n",
      "Eval num_timesteps=820000, episode_reward=92.00 +/- 55.01\n",
      "Episode length: 627.40 +/- 236.42\n",
      "Eval num_timesteps=840000, episode_reward=82.00 +/- 50.36\n",
      "Episode length: 552.40 +/- 98.31\n",
      "Eval num_timesteps=860000, episode_reward=110.00 +/- 21.68\n",
      "Episode length: 730.00 +/- 79.94\n",
      "Eval num_timesteps=880000, episode_reward=78.00 +/- 17.78\n",
      "Episode length: 568.00 +/- 92.82\n",
      "Eval num_timesteps=900000, episode_reward=27.00 +/- 13.64\n",
      "Episode length: 535.60 +/- 91.79\n",
      "Eval num_timesteps=920000, episode_reward=170.00 +/- 158.02\n",
      "Episode length: 867.40 +/- 273.60\n",
      "Eval num_timesteps=940000, episode_reward=161.00 +/- 118.76\n",
      "Episode length: 698.20 +/- 212.47\n",
      "Eval num_timesteps=960000, episode_reward=154.00 +/- 77.16\n",
      "Episode length: 743.00 +/- 226.08\n",
      "Eval num_timesteps=980000, episode_reward=122.00 +/- 72.50\n",
      "Episode length: 768.80 +/- 175.01\n",
      "Eval num_timesteps=1000000, episode_reward=169.00 +/- 68.22\n",
      "Episode length: 811.00 +/- 183.86\n"
     ]
    }
   ],
   "source": [
    "#train the agent\n",
    "SpaceInvaders_agent.train(time_steps=1000000, stop_value=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wk6znVXONYMq",
    "outputId": "7ec15a9a-8057-4b77-da1e-e99e250d6478"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward over 10 episodes is 236.5 with a standard deviation of 159.42161083115425\n"
     ]
    }
   ],
   "source": [
    "SpaceInvaders_agent.evaluate_policy(episodes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "nX3848uYH1Wv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: 265.0\n",
      "Episode 2: 235.0\n",
      "Episode 3: 205.0\n",
      "Episode 4: 235.0\n",
      "Episode 5: 140.0\n",
      "Episode 6: 170.0\n",
      "Episode 7: 85.0\n",
      "Episode 8: 185.0\n",
      "Episode 9: 90.0\n",
      "Episode 10: 210.0\n",
      "Moviepy - Building video c:\\Users\\Steel\\Downloads\\my_atari_games\\Saved_Models\\DQN_SpaceInvaders_Model\\SpaceInvaders_Agent_play/SpaceInvaders-agent-play-episode-0.mp4.\n",
      "Moviepy - Writing video c:\\Users\\Steel\\Downloads\\my_atari_games\\Saved_Models\\DQN_SpaceInvaders_Model\\SpaceInvaders_Agent_play/SpaceInvaders-agent-play-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready c:\\Users\\Steel\\Downloads\\my_atari_games\\Saved_Models\\DQN_SpaceInvaders_Model\\SpaceInvaders_Agent_play/SpaceInvaders-agent-play-episode-0.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# test out the agent with the space invaders game\n",
    "SpaceInvaders_agent.play_episodes(num_episodes=10, play_type=\"predict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "f3Gl36T4MTb0"
   },
   "outputs": [],
   "source": [
    "#Close the environment\n",
    "SpaceInvaders_agent.close_env()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2TptTs3RzbZp"
   },
   "source": [
    "# Pacman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "8bi599sNy-Sc"
   },
   "outputs": [],
   "source": [
    "#initialize the agent and create the environment\n",
    "Pacman_agent_agent = DQNAgent(name=\"Pacman\", env_name=\"MsPacman-v4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "xN0bOZrEy-bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Playing the Pacman game randomly for 20 episodes\n",
      "Episode 1: 210.0\n",
      "Episode 2: 200.0\n",
      "Episode 3: 240.0\n",
      "Episode 4: 400.0\n",
      "Episode 5: 160.0\n",
      "Episode 6: 150.0\n",
      "Episode 7: 190.0\n",
      "Episode 8: 190.0\n",
      "Episode 9: 140.0\n",
      "Episode 10: 220.0\n",
      "Episode 11: 220.0\n",
      "Episode 12: 140.0\n",
      "Episode 13: 210.0\n",
      "Episode 14: 250.0\n",
      "Episode 15: 230.0\n",
      "Episode 16: 160.0\n",
      "Episode 17: 150.0\n",
      "Episode 18: 230.0\n",
      "Episode 19: 270.0\n",
      "Episode 20: 190.0\n"
     ]
    }
   ],
   "source": [
    "#Play the pacman game randomly for 20 episodes\n",
    "Pacman_agent_agent.play_episodes(num_episodes=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "lOo_tlGiy-oh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=20000, episode_reward=486.00 +/- 79.90\n",
      "Episode length: 861.60 +/- 102.27\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=502.00 +/- 62.42\n",
      "Episode length: 695.20 +/- 74.50\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=542.00 +/- 94.32\n",
      "Episode length: 629.20 +/- 92.42\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=518.00 +/- 151.97\n",
      "Episode length: 750.20 +/- 185.25\n",
      "Eval num_timesteps=100000, episode_reward=646.00 +/- 220.87\n",
      "Episode length: 831.20 +/- 86.72\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=388.00 +/- 76.00\n",
      "Episode length: 638.00 +/- 125.80\n",
      "Eval num_timesteps=140000, episode_reward=376.00 +/- 109.65\n",
      "Episode length: 794.40 +/- 97.70\n",
      "Eval num_timesteps=160000, episode_reward=482.00 +/- 99.28\n",
      "Episode length: 730.40 +/- 133.74\n",
      "Eval num_timesteps=180000, episode_reward=510.00 +/- 374.01\n",
      "Episode length: 829.00 +/- 204.96\n",
      "Eval num_timesteps=200000, episode_reward=862.00 +/- 273.31\n",
      "Episode length: 922.20 +/- 157.37\n",
      "New best mean reward!\n",
      "Eval num_timesteps=220000, episode_reward=478.00 +/- 98.67\n",
      "Episode length: 754.80 +/- 136.67\n",
      "Eval num_timesteps=240000, episode_reward=508.00 +/- 79.09\n",
      "Episode length: 781.00 +/- 124.02\n",
      "Eval num_timesteps=260000, episode_reward=466.00 +/- 211.72\n",
      "Episode length: 640.00 +/- 133.11\n",
      "Eval num_timesteps=280000, episode_reward=558.00 +/- 257.48\n",
      "Episode length: 775.20 +/- 156.19\n",
      "Eval num_timesteps=300000, episode_reward=358.00 +/- 72.22\n",
      "Episode length: 708.00 +/- 261.92\n",
      "Eval num_timesteps=320000, episode_reward=296.00 +/- 134.10\n",
      "Episode length: 689.80 +/- 115.27\n",
      "Eval num_timesteps=340000, episode_reward=340.00 +/- 41.47\n",
      "Episode length: 703.00 +/- 81.76\n",
      "Eval num_timesteps=360000, episode_reward=448.00 +/- 59.80\n",
      "Episode length: 699.20 +/- 87.12\n",
      "Eval num_timesteps=380000, episode_reward=568.00 +/- 44.00\n",
      "Episode length: 666.20 +/- 80.61\n",
      "Eval num_timesteps=400000, episode_reward=1550.00 +/- 1243.64\n",
      "Episode length: 837.00 +/- 269.63\n",
      "New best mean reward!\n"
     ]
    }
   ],
   "source": [
    "#train the agent\n",
    "Pacman_agent_agent.train(time_steps=1000000, stop_value=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "X02-fj5RzPQT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward over 10 episodes is 728.0 with a standard deviation of 382.1465687403198\n"
     ]
    }
   ],
   "source": [
    "#evaluate the policy used by the agent\n",
    "Pacman_agent_agent.evaluate_policy(episodes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "zEwXh6nO1BYo"
   },
   "outputs": [],
   "source": [
    "Pacman_agent_agent.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "TWssbaeszTMi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: 410.0\n",
      "Episode 2: 480.0\n",
      "Episode 3: 820.0\n",
      "Episode 4: 660.0\n",
      "Episode 5: 800.0\n",
      "Episode 6: 410.0\n",
      "Episode 7: 430.0\n",
      "Episode 8: 1050.0\n",
      "Episode 9: 530.0\n",
      "Episode 10: 620.0\n",
      "Moviepy - Building video c:\\Users\\Steel\\Downloads\\my_atari_games\\Saved_Models\\DQN_Pacman_Model\\Pacman_Agent_play/Pacman-agent-play-episode-0.mp4.\n",
      "Moviepy - Writing video c:\\Users\\Steel\\Downloads\\my_atari_games\\Saved_Models\\DQN_Pacman_Model\\Pacman_Agent_play/Pacman-agent-play-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready c:\\Users\\Steel\\Downloads\\my_atari_games\\Saved_Models\\DQN_Pacman_Model\\Pacman_Agent_play/Pacman-agent-play-episode-0.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# test out the agent with the pacman game\n",
    "Pacman_agent_agent.play_episodes(num_episodes=10, play_type=\"predict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "O8kAQe2PzX2T"
   },
   "outputs": [],
   "source": [
    "#Close the environment\n",
    "Pacman_agent_agent.close_env()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
